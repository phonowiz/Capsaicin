/**********************************************************************
Copyright (c) 2025 Advanced Micro Devices, Inc. All rights reserved.
********************************************************************/

#include "nrc_common.hlsl"

// Enable 16-bit types
typedef half  float16_t;
typedef half2 float16_t2;

#define LAYER_WIDTH_HALVES 32   
#define THREAD_NEURONS_HALVES 8   
#define BLOCK_PIXELS 128        

ConstantBuffer<NRCConstants> g_NRCConstants : register(b0);

// SRVs
StructuredBuffer<InferenceQuery> g_InferenceQueries : register(t0);
StructuredBuffer<float16_t2>    g_Weights          : register(t1);
StructuredBuffer<TrainingSample> g_TrainingSamples  : register(t2);
StructuredBuffer<uint>           g_Counters         : register(t3);

// UAVs
RWTexture2D<float4>            g_OutputTexture      : register(u1);
RWStructuredBuffer<float16_t2> g_Activations        : register(u2);
RWStructuredBuffer<uint>       g_WeightGradients    : register(u3); // Weight Gradients (packed half2 for atomics)
RWStructuredBuffer<NrcFloat>   g_Momentum1           : register(u4);
RWStructuredBuffer<NrcFloat>   g_Momentum2           : register(u5);
RWTexture2D<float4>            g_AccumulationBuffer : register(u6);
RWStructuredBuffer<float16_t2> g_IncomingGradients  : register(u7); // Loss gradients

groupshared float16_t2 s_activations[LAYER_WIDTH_HALVES][BLOCK_PIXELS];
groupshared float16_t2 s_weights[LAYER_WIDTH_HALVES * 64];
groupshared float16_t2 s_weights_T[LAYER_WIDTH_HALVES * 64]; 
groupshared float16_t2 s_dL_da[LAYER_WIDTH_HALVES][BLOCK_PIXELS];

// Helper for atomic addition of packed half2
void InterlockedAddHalf2(uint address, float16_t2 value)
{
    uint current = g_WeightGradients[address];
    uint expected;
    [allow_uav_condition]
    do {
        expected = current;
        // Bit-accurate conversion from packed uint to float16_t2
        float2 f32 = float2(f16tof32(current & 0xFFFF), f16tof32(current >> 16));
        float16_t2 sum = (float16_t2)f32 + value;
        uint next = f32tof16(sum.x) | (f32tof16(sum.y) << 16);
        InterlockedCompareExchange(g_WeightGradients[address], expected, next, current);
    } while (current != expected);
}

[numthreads(BLOCK_PIXELS, 1, 1)]
void NRCInference(uint3 dtid : SV_DispatchThreadID, uint3 gtid : SV_GroupThreadID)
{
    const uint pixel_in_block = gtid.x;
    const uint warp_id = gtid.x / 32;
    const uint global_pixel_idx = dtid.x;

    uint count = g_Counters[0];
    if (global_pixel_idx >= count)
        return;

    InferenceQuery query = g_InferenceQueries[global_pixel_idx];
    
    // 1. Encode Input
    float activations_f[NETWORK_WIDTH];
    EncodeInputs(query, activations_f);
    
    // Store initial encoded features into Shared Memory
    [unroll]
    for (int i = 0; i < LAYER_WIDTH_HALVES; i++)
    {
        s_activations[i][pixel_in_block] = float16_t2(
            (float16_t)activations_f[i * 2], 
            (float16_t)activations_f[i * 2 + 1]
        );
    }

    // --- MLP Layers ---
    [loop]
    for (int layer = 0; layer < 7; layer++)
    {
        // A. LOG ACTIVATIONS (Before they are overwritten by matrix multiply)
        // These are the inputs to the current 'layer'
        // [unroll]
        for (int k = 0; k < LAYER_WIDTH_HALVES; k++)
        {
            uint act_idx = (layer * g_NRCConstants.num_inference_queries + global_pixel_idx) * LAYER_WIDTH_HALVES + k;
            g_Activations[act_idx] = s_activations[k][pixel_in_block];
        }

        const int LAYER_WEIGHT_STRIDE = NETWORK_WIDTH * LAYER_WIDTH_HALVES;
        // B. Cooperative Weight Load
        [unroll]
        for (int j = 0; j < 16; j++)
        {
            int w_idx = j * BLOCK_PIXELS + pixel_in_block;
            s_weights[w_idx] = g_Weights[layer * LAYER_WEIGHT_STRIDE + w_idx];
        }
        GroupMemoryBarrierWithGroupSync();

        // C. Compute (Fused half2 Multiply-Accumulate)
        float16_t2 my_results[THREAD_NEURONS_HALVES]; 
        [unroll]
        for (int out_pair = 0; out_pair < THREAD_NEURONS_HALVES; out_pair++)
        {
            float16_t2 sum2 = (float16_t2)0.0;
            int row_index = (warp_id * THREAD_NEURONS_HALVES) + out_pair;

            [unroll]
            for (int in_pair = 0; in_pair < LAYER_WIDTH_HALVES; in_pair++)
            {
                sum2 += s_activations[in_pair][pixel_in_block] * s_weights[row_index * LAYER_WIDTH_HALVES + in_pair];
            }
            // ReLU
            my_results[out_pair] = max(sum2, (float16_t2)0.0);
        }

        // D. Hand-off (Write the new activations for the next loop/layer)
        GroupMemoryBarrierWithGroupSync();
        [unroll]
        for (int out_p = 0; out_p < THREAD_NEURONS_HALVES; out_p++)
        {
            int r_idx = (warp_id * THREAD_NEURONS_HALVES) + out_p;
            s_activations[r_idx][pixel_in_block] = my_results[out_p];
        }
        GroupMemoryBarrierWithGroupSync();
    }

    if(warp_id == 0)
    {
        // 3. Final Write-out (Reducing 64 neurons to RGB and Factorizing)
        float16_t2 rg_raw = s_activations[0][pixel_in_block];
        float16_t  b_raw  = s_activations[1][pixel_in_block].x;

        float3 radiance = float3((float)rg_raw.x, (float)rg_raw.y, (float)b_raw);
        radiance = max(0.0f, radiance);
        
        float3 final_color = radiance * (query.albedo.rgb /*+ query.specular*/);
        
        // Accumulation Buffer Logic
        //float4 acc = g_AccumulationBuffer[query.pixel_coord];
        //float3 current_radiance = acc.xyz;
        //uint sample_count = asuint(acc.w);
        
        //if (sample_count > 0)
        //{
        //    float3 updated = current_radiance + (final_color * query.throughput / (float)sample_count);
        //    g_AccumulationBuffer[query.pixel_coord] = float4(updated, asfloat(sample_count));
        //}
        
        g_OutputTexture[query.pixel_coord] = float4(final_color, 1.0f);
    }
}

// Forward Pass
// Uses Shared Memory for weights to reuse across thread block.
// Groupshared: We need to load 64x64 float weights. 4KB. Fits easily.
// groupshared NrcFloat s_LayerWeights[NETWORK_WIDTH * NETWORK_WIDTH];

// [numthreads(GROUP_SIZE, 1, 1)]
// void NRCInference(uint3 dtid : SV_DispatchThreadID, uint3 gtid : SV_GroupThreadID)
// {
//     uint count = g_Counters[0];
//     if (dtid.x >= count) return; // Dynamic count check

//     InferenceQuery query = g_InferenceQueries[dtid.x];
    
//     // 1. Encode
//     NrcFloat activations[NETWORK_WIDTH][GROUP_SIZE];
//     //EncodeInput(query, activations);
//     EncodeInputs(query, activations);
    
//     // 2. Run Layers
//     for (int layer = 0; layer < HIDDEN_LAYERS; ++layer) // +2 for In/Out?
//     {
//         // For simplicity, let's do 5 layers of 64x64.
//         // Load Weights into Shared Memory cooperatively
//         // Total weights: 64*64 = 4096.
//         // Threads: 128. Each thread loads 32 floats.
//         uint weight_offset = layer * NETWORK_WIDTH * NETWORK_WIDTH;
        
//         for (int i = 0; i < 32; ++i)
//         {
//             int idx = i * GROUP_SIZE + gtid.x;
//             if (idx < NETWORK_WIDTH * NETWORK_WIDTH)
//             {
//                 s_LayerWeights[idx] = g_Weights[weight_offset + idx];
//             }
//         }
//         GroupMemoryBarrierWithGroupSync();
        
//         // Matrix Multiply: Output = Activation * Weight
//         // We calculate new activations for this thread
//         NrcFloat next_activations[NETWORK_WIDTH];
        
//         for (int out_row = 0; out_row < NETWORK_WIDTH; ++out_row)
//         {
//             NrcFloat sum = 0.0f;
//             for (int in_col = 0; in_col < NETWORK_WIDTH; ++in_col)
//             {
//                 // W is stored Row-Major? Let's assume W[row][col]
//                 // Output[row] = Sum(Input[col] * W[row][col])
//                 sum += activations[in_col] * s_LayerWeights[out_row * NETWORK_WIDTH + in_col];
//             }
            
//             // ReLU (except last layer?)
//             // For now, ReLU all hidden.
//             if (layer < HIDDEN_LAYERS)
//                  next_activations[out_row] = max(0.0f, sum);
//             else
//                  next_activations[out_row] = sum; // Sigmoid or Linear for last?
//         }
        
//         // Update registers
//         activations = next_activations;
        
//         GroupMemoryBarrierWithGroupSync();
//     }
    
//     // 3. Output
//     // First 3 floats are RGB.
//     // Factorize with Albedo (from query).
//     float3 radiance = float3(activations[0], activations[1], activations[2]);
    
//     // Apply albedo factorization (demodulation)
//     // In paper: Network predicts "Radiance / Albedo" (plus some factor), so we multiply back.
//     // Ensure positive
//     radiance = max(0.0f, radiance);
    
//     // Write Output
//     // Simple Exponential accumulation check
//     float3 final_color = radiance * (query.albedo + 0.001f); // Multiply by albedo
    
//     // Add to Accumulation
//     // Note: This is racy if multiple queries hit same pixel? 
//     // Usually 1 query per pixel if bounce=2.
//     // If we have jitter, we need to access the AccBuffer carefully.
//     // But let's assume standard accumulation logic happens in RayGen for primary, here for indirect.
//     // "radiance" here is L_indirect.
//     // L_total = L_primary + Throughput * L_indirect
    
//     float4 acc = g_AccumulationBuffer[query.pixel_coord];
//     float3 current_radiance = acc.xyz;
//     uint sample_count = asuint(acc.w);
    
//     // Weighted update? The RayGen already divided L_primary by N.
//     // We should divide this by N too.
//     // BUT RayGen did: rad = rad_prev + (new - rad_prev)/N.
//     // It's easier if RayGen writes (L_primary) and we ADD (Throughput * L_nrc / N).
//     // Or we just strictly add to the current frame's radiance before averaging?
//     // Let's assume RayGen wrote `Prev + (Primary - Prev)/N`.
//     // We want `Prev + ((Primary + T*NRC) - Prev)/N`.
//     // Difference is `(T*NRC)/N`.
    
//     if (sample_count > 0)
//         g_AccumulationBuffer[query.pixel_coord] = float4(current_radiance + (final_color * query.throughput / (float)sample_count), asfloat(sample_count));
    
//     g_OutputTexture[query.pixel_coord] = float4(final_color, 1.0f);
// }

// Training Kernel
// Naive implementation: One thread per sample. 
// Standard backprop.
// [numthreads(GROUP_SIZE, 1, 1)]
// void NRCTrain(uint3 dtid : SV_DispatchThreadID)
// {
//     uint count = g_Counters[1];
//     if (dtid.x >= count) return;
    
//     TrainingSample sample = g_TrainingSamples[dtid.x];
    
//     // Forward Pass (Record activations needed for backprop)
//     // We need to store activations for all layers for this sample.
//     // Stack: [Layers][Width]
//     float layer_activations[HIDDEN_LAYERS + 1][NETWORK_WIDTH];
    
//     EncodeInputTraining(sample, layer_activations[0]);
    
//     //[unroll]
//     //for (int i = 0; i < LAYER_WIDTH_HALVES; i++)
//     //{
//     //    s_activations[i][pixel_in_block] = float16_t2(
//     //        (float16_t) layer_activations[0][i * 2],
//     //        (float16_t) layer_activations[0][i * 2 + 1]
//     //    );
//     //}
    
//     for (int layer = 0; layer < HIDDEN_LAYERS; ++layer)
//     {
//          uint weight_offset = layer * NETWORK_WIDTH * NETWORK_WIDTH;
         
//          for (int r = 0; r < NETWORK_WIDTH; ++r)
//          {
//              float16_t sum = 0.0f;
//              for (int c = 0; c < NETWORK_WIDTH; ++c)
//              {
//                  sum += layer_activations[layer][c] * g_Weights[weight_offset + r * NETWORK_WIDTH + c];
//              }
//              layer_activations[layer+1][r] = max(0.0f, sum); // ReLU
//          }
//     }
    
//     // Output Layer (Last transform)
//     // This is simplified. Normally output is smaller (3 dims).
//     // Let's assume Layer 5 maps 64 -> 64, and we just take first 3.
//     // Real implementation would have a specific output layer size.
    
//     // Calc Loss Gradients
//     float3 prediction = float3(layer_activations[HIDDEN_LAYERS][0], layer_activations[HIDDEN_LAYERS][1], layer_activations[HIDDEN_LAYERS][2]);
//     float3 target = sample.target_radiance; // This should be Demodulated Radiance (Radiance / Albedo)
    
//     // L2 Loss: 0.5 * (Pred - Target)^2
//     // Grad: (Pred - Target)
//     float3 dL_dPred = prediction - target;
    
//     // Backprop
//     NrcFloat dL_dX[NETWORK_WIDTH]; // Gradients for current layer neurons
    
//     // Init dL_dX for output layer
//     for(int i=0; i<NETWORK_WIDTH; ++i) dL_dX[i] = 0.0f;
//     dL_dX[0] = dL_dPred.x;
//     dL_dX[1] = dL_dPred.y;
//     dL_dX[2] = dL_dPred.z;
    
//     // Loop backwards
//     for (int layer = HIDDEN_LAYERS - 1; layer >= 0; --layer)
//     {
//          uint weight_offset = layer * NETWORK_WIDTH * NETWORK_WIDTH;
//          NrcFloat dL_dX_prev[NETWORK_WIDTH]; // For input of this layer
//          for(int k=0; k<NETWORK_WIDTH; ++k) dL_dX_prev[k] = 0.0f;
         
//          for (int r = 0; r < NETWORK_WIDTH; ++r)
//          {
//              // If ReLU was active
//              NrcFloat output_val = layer_activations[layer+1][r];
//              NrcFloat grad = (output_val > 0.0f) ? dL_dX[r] : 0.0f;
             
//              // Update Weights Gradients
//              // dL_dW_rc = dL_dOutput_r * Input_c
//              // Atomic Add to Global Gradients? VERY SLOW.
//              // Better: Compute local gradients, then do a reduction or have one thread do a batch?
//              // Standard SGD is noisy.
//              // For this naive implementation, we will perform atomic adds.
//              // Warning: This is a performance killer. Real implementation uses warp-shuffles to reduce.
             
//              for (int c = 0; c < NETWORK_WIDTH; ++c)
//              {
//                  NrcFloat w_grad = grad * layer_activations[layer][c];
//                  int w_idx = weight_offset + r * NETWORK_WIDTH + c;
                 
//                  // Using InterlockedAddFloat equivalent (requires CAS loop if not widely supported)
//                  // or just write to a huge buffer [BatchSize][Weights] and reduce later.
//                  // Let's assume we can tolerate race conditions for HOGWILD-style SGD,
//                  // or we use a very small batch size per frame and do it serially?
//                  // No, we need atomics.
                 
//                  // Placeholder for Atomic Float Add:
//                  // AtomicAddFloat(g_Gradients[w_idx], w_grad);
//              }
             
//              // Compute gradient for previous layer
//              // dL_dX_prev_c += dL_dOutput_r * W_rc
//              for (int c = 0; c < NETWORK_WIDTH; ++c)
//              {
//                  dL_dX_prev[c] += grad * g_Weights[weight_offset + r * NETWORK_WIDTH + c];
//              }
//          }
         
//          // Move to next
//          for(int k=0; k<NETWORK_WIDTH; ++k) dL_dX[k] = dL_dX_prev[k];
//     }
// }


[numthreads(BLOCK_PIXELS, 1, 1)]
void NRCTrain(uint3 dtid : SV_DispatchThreadID, uint3 gtid : SV_GroupThreadID, uint3 ctid : SV_GroupID)
{
    const int pixel_in_block = gtid.x;
    const int warp_id = gtid.x / 32;
    const int lane_id = gtid.x % 32;
    const int global_pixel_idx = dtid.x;
    const uint total_samples = g_Counters[1];

    if (global_pixel_idx >= total_samples)
        return;

    // 1. Initial Load: Gradients from the output layer
    [unroll]
    for (int i = 0; i < LAYER_WIDTH_HALVES; i++)
    {
        s_dL_da[i][pixel_in_block] = g_IncomingGradients[global_pixel_idx * LAYER_WIDTH_HALVES + i];
    }
    GroupMemoryBarrierWithGroupSync();

    // 2. Backprop Loop (7 layers)
    [loop]
    for (int layer = 6; layer >= 0; layer--)
    {
        // --- A. LOAD PHASE ---
        // Load activations from forward pass
        [unroll]
        for (int i = 0; i < LAYER_WIDTH_HALVES; i++)
        {
            uint act_idx = (layer * g_NRCConstants.num_training_samples + global_pixel_idx) * LAYER_WIDTH_HALVES + i;
            s_activations[i][pixel_in_block] = g_Activations[act_idx];
        }

        // Load weights and TRANSPOSE them into shared memory
        [unroll]
        for (int i = 0; i < 16; i++)
        {
            int linear_idx = (i * 128) + pixel_in_block;
            int row = linear_idx / 32;
            int col = linear_idx % 32;
            // Store W[row][col] into smem[col][row]
            s_weights_T[col * 64 + row] = g_Weights[layer * 2048 + linear_idx];
        }
        GroupMemoryBarrierWithGroupSync();

        // --- B. COMPUTE WEIGHT GRADIENTS (dl_dW = X^T * dl_da) ---
        float16_t2 local_delta_w[THREAD_NEURONS_HALVES];
        [unroll]
        for (int i = 0; i < THREAD_NEURONS_HALVES; i++) 
            local_delta_w[i] = (float16_t2)0.0;

        [loop]
        for (int pix = 0; pix < BLOCK_PIXELS; pix++)
        {
            // If the pixel in the block is out of bounds for the entire batch, skip its contribution
            if (ctid.x * BLOCK_PIXELS + pix >= total_samples) continue;

            float16_t2 grad = s_dL_da[lane_id][pix];
            [unroll]
            for (int i = 0; i < THREAD_NEURONS_HALVES; i++)
            {
                int row_idx = (warp_id * THREAD_NEURONS_HALVES) + i;
                float16_t2 act = s_activations[row_idx][pix];
                local_delta_w[i] += act * grad;
            }
        }

        // Write weight gradients to global memory using atomics
        [unroll]
        for (int i = 0; i < THREAD_NEURONS_HALVES; i++)
        {
            int row = (warp_id * THREAD_NEURONS_HALVES) + i;
            int global_idx = layer * 2048 + (row * 32 + lane_id);
            InterlockedAddHalf2(global_idx, local_delta_w[i]);
        }

        // --- C. COMPUTE INPUT GRADIENTS (dL/dX = G * W^T) with ReLU Derivative ---
        float16_t2 next_grads[THREAD_NEURONS_HALVES];
        [unroll]
        for (int in_p = 0; in_p < THREAD_NEURONS_HALVES; in_p++)
        {
            float16_t2 g_sum = (float16_t2)0.0;
            [unroll]
            for (int out_p = 0; out_p < LAYER_WIDTH_HALVES; out_p++)
            {
                float16_t2 w_t = s_weights_T[in_p * 64 + out_p];
                float16_t2 g_up = s_dL_da[out_p][pixel_in_block];
                g_sum += g_up * w_t;
            }
            
            int act_idx = (warp_id * THREAD_NEURONS_HALVES) + in_p;
            float16_t2 fwd_act = s_activations[act_idx][pixel_in_block];
            float16_t2 mask = (float16_t2)(fwd_act > (float16_t2)0.0);
            next_grads[in_p] = g_sum * mask;
        }

        // --- D. HAND-OFF ---
        GroupMemoryBarrierWithGroupSync();
        [unroll]
        for (int i = 0; i < THREAD_NEURONS_HALVES; i++)
        {
            int row_idx = (warp_id * THREAD_NEURONS_HALVES) + i;
            s_dL_da[row_idx][pixel_in_block] = next_grads[i];
        }
        GroupMemoryBarrierWithGroupSync();
    }
}
