/**********************************************************************
NRC Training Shader - Full Implementation with Detailed Striding Logic
Copyright (c) 2026 AMD / NRC Implementation
********************************************************************/

#include "nrc_common.hlsl"

// Enable 16-bit types for modern GPU hardware acceleration
typedef half  float16_t;
typedef half2 float16_t2;

// Paper Specifics: 64 neurons, 7 hidden layers
#define LAYER_WIDTH_HALVES 32   // (64 neurons / 2)
#define BLOCK_PIXELS 64         // 64 samples processed per thread block
#define WEIGHTS_PER_LAYER 1024  // 32 * 32 float16_t2 (representing 64x64 weights)

ConstantBuffer<NRCConstants> g_NRCConstants : register(b0);

// Resources
StructuredBuffer<float16_t2>    g_Weights           : register(t1);
StructuredBuffer<uint>          g_Counters          : register(t3);
RWStructuredBuffer<float16_t2>  g_Activations       : register(u2);
RWStructuredBuffer<uint>        g_WeightGradients   : register(u3); 
RWStructuredBuffer<float16_t2>  g_IncomingGradients : register(u7); 

// Shared Memory (Approx 12KB)
groupshared float16_t2 s_activations[LAYER_WIDTH_HALVES][BLOCK_PIXELS];
groupshared float16_t2 s_dL_da[LAYER_WIDTH_HALVES][BLOCK_PIXELS];
groupshared uint       s_grad_accel[WEIGHTS_PER_LAYER]; 

// --- Helpers for Atomic Half2 Addition ---

void AtomicAddHalf2Shared(uint addr, float16_t2 val) {
    uint current;
    InterlockedCompareExchange(s_grad_accel[addr], 0, 0, current);
    [allow_uav_condition]
    while (true) {
        uint expected = current;
        // Convert to float32 for summation to avoid precision drift/stalling
        float2 f32 = float2(f16tof32(current & 0xFFFF), f16tof32(current >> 16)) + (float2)val;
        uint next = f32tof16(f32.x) | (f32tof16(f32.y) << 16);
        InterlockedCompareExchange(s_grad_accel[addr], expected, next, current);
        if (current == expected) break;
    }
}

void AtomicAddHalf2Global(uint addr, float16_t2 val) {
    uint current;
    InterlockedCompareExchange(g_WeightGradients[addr], 0, 0, current);
    [allow_uav_condition]
    while (true) {
        uint expected = current;
        float2 f32 = float2(f16tof32(current & 0xFFFF), f16tof32(current >> 16)) + (float2)val;
        uint next = f32tof16(f32.x) | (f32tof16(f32.y) << 16);
        InterlockedCompareExchange(g_WeightGradients[addr], expected, next, current);
        if (current == expected) break;
    }
}

// --- Main Training Kernel ---

[numthreads(BLOCK_PIXELS, 1, 1)]
void NRCTrain(uint3 dtid : SV_DispatchThreadID, uint3 gtid : SV_GroupThreadID, uint3 ctid : SV_GroupID)
{
    const int t_idx = gtid.x;
    const uint total_samples = g_Counters[1];

    if(total_samples == 0)
        return;
    const int global_sample_idx = dtid.x;

    // --- STRIDING LOGIC EXPLAINED ---
    // We have 64 threads (t_idx 0-63) but only 32 rows of data (LAYER_WIDTH_HALVES).
    // threads_per_row = 2.
    const int threads_per_row = BLOCK_PIXELS / LAYER_WIDTH_HALVES;
    
    // worker_id is either 0 or 1. This distinguishes the two threads assigned to the same row.
    const int worker_id = t_idx % threads_per_row;
    
    // row_idx (0-31) tells the thread which neuron's output error it is processing.
    const int row_idx   = t_idx / threads_per_row;

    // 1. Initial Gradient Load (dL/dy)
    if (global_sample_idx < total_samples) {
        [unroll]
        for (int i = 0; i < LAYER_WIDTH_HALVES; i++)
            s_dL_da[i][t_idx] = g_IncomingGradients[global_sample_idx * LAYER_WIDTH_HALVES + i];
    } else {
        [unroll]
        for (int i = 0; i < LAYER_WIDTH_HALVES; i++) s_dL_da[i][t_idx] = (float16_t2)0;
    }
    GroupMemoryBarrierWithGroupSync();

    // 2. Backpropagation Loop (Iterate 7 layers: 6 to 0)
    [loop]
    for (int layer = 6; layer >= 0; layer--)
    {
        // A. Reset Shared Accumulator
        for (int c = t_idx; c < WEIGHTS_PER_LAYER; c += BLOCK_PIXELS) 
            s_grad_accel[c] = 0;
        
        // B. THE TRANSPOSE LOAD
        // -------------------------------------------------------------------------
        // In Global Memory, data is grouped by pixel: [P0_N0, P0_N1, P0_N2...]
        // We want Shared Memory to be grouped by neuron: [N0_P0, N0_P1, N0_P2...]
        // -------------------------------------------------------------------------
        if (global_sample_idx < total_samples) {
            [unroll]
            for (int neuron = 0; neuron < LAYER_WIDTH_HALVES; neuron++)
            {
                // Index into global memory (Pixel-Major)
                uint global_idx = (layer * g_NRCConstants.activations_stride +  
                                   global_sample_idx) * LAYER_WIDTH_HALVES + neuron;
                
                // Write into shared memory (Neuron-Major / Transposed)
                // Note how 'i' (neuron) is the first index, 't_idx' (pixel) is the second.
                s_activations[neuron][t_idx] = g_Activations[global_idx];
            }
        }
        GroupMemoryBarrierWithGroupSync();

        // C. Weight Gradient Accumulation (The Outer Product: DeltaW = G * X^T)
        // -------------------------------------------------------------------------
        // Instead of having 1 thread process 64 pixels (too slow),
        // we have 2 threads process 32 pixels each for a specific row.
        // Thread(worker_id=0) starts at p=0, steps to 2, 4, 6...
        // Thread(worker_id=1) starts at p=1, steps to 3, 5, 7...
        // -------------------------------------------------------------------------

        // Weight Gradient Accumulation
        // -------------------------------------------------------------------------
        // FORMULA: dL/dW = Sum_{p=0}^{N-1} ( G_p * X_p^T )
        // Where:
        //   dL/dW = Gradient for the weight matrix (64x64)
        //   G_p   = Error gradient vector for pixel 'p' (Column vector 64x1)
        //   X_p^T = Forward activations vector for pixel 'p' (Row vector 1x64)
        //
        // In this implementation:
        //   Thread 't_idx' handles Row 'row_idx' of the matrix.
        //   Two threads (worker_id 0 and 1) split the Summation loop (p += 2).
        // -------------------------------------------------------------------------
        [loop]
        for (int p = worker_id; p < BLOCK_PIXELS; p += threads_per_row) 
        {
            float16_t2 G = s_dL_da[row_idx][p]; // dL/da (Error gradient for this row/pixel)
            
            if (any(G != (float16_t2)0)) {
                [unroll]
                for (int col = 0; col < LAYER_WIDTH_HALVES; col++) {
                    float16_t2 X = s_activations[col][p]; // Input activation for this column/pixel
                    float16_t2 dW_contribution = G * X;   // Scalar multiply (Outer Product component)
                    
                    AtomicAddHalf2Shared(row_idx * LAYER_WIDTH_HALVES + col, dW_contribution);
                }
            }
        }
        GroupMemoryBarrierWithGroupSync();

        // D. Input Gradient Calculation (dL/dX = W^T * G)
        // Transposes the weight matrix during the multiply to push error back a layer.
        float16_t2 next_grad[LAYER_WIDTH_HALVES];
        [unroll]
        for (int i = 0; i < LAYER_WIDTH_HALVES; i++) {
            float16_t2 sum = (float16_t2)0;
            [unroll]
            for (int j = 0; j < LAYER_WIDTH_HALVES; j++) {
                float16_t2 w = g_Weights[layer * WEIGHTS_PER_LAYER + j * LAYER_WIDTH_HALVES + i];
                sum += w * s_dL_da[j][t_idx];
            }
            // Apply ReLU derivative: Gradient only passes if neuron was active (>0)
            float16_t2 mask = (float16_t2)(s_activations[i][t_idx] > (float16_t2)0);
            next_grad[i] = sum * mask;
        }

        // E. Flush Block Gradients to Global Buffer
        // This moves the local 64-sample sum to the global training-wide sum.
        for (int g = t_idx; g < WEIGHTS_PER_LAYER; g += BLOCK_PIXELS) {
            uint val = s_grad_accel[g];
            float16_t2 f16_val = float16_t2((float16_t)f16tof32(val & 0xFFFF), (float16_t)f16tof32(val >> 16));
            AtomicAddHalf2Global(layer * WEIGHTS_PER_LAYER + g, f16_val);
        }

        // F. Iteration Hand-off
        GroupMemoryBarrierWithGroupSync();
        [unroll]
        for (int i = 0; i < LAYER_WIDTH_HALVES; i++) s_dL_da[i][t_idx] = next_grad[i];
        GroupMemoryBarrierWithGroupSync();
    }
}
